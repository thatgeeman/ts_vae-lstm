{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ad_complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime as dt\n",
    "from ts_vae_lstm.concepts import get_window\n",
    "from scipy import signal\n",
    "import os\n",
    "import math\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from ts_vae_lstm.vae import VAE, Encoder, Decoder, StochasticSampler\n",
    "from fastcore.xtras import noop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 'cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_workers = os.cpu_count()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_workers, device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a VAE with no anomalies in the time series.\n",
    "\n",
    "```--> Anomalies occur at:\n",
    "  timestamp #0: 2014-11-01 19:00:00\n",
    "  timestamp #1: 2014-11-27 15:30:00\n",
    "  timestamp #2: 2014-12-25 15:00:00\n",
    "  timestamp #3: 2015-01-01 01:00:00\n",
    "  timestamp #4: 2015-01-27 00:00:00\n",
    "\n",
    "Original csv file contains (10320,) timestamps.\n",
    "Processed time series contain (10320,) readings.\n",
    "Anomaly indices are [5943, 7184, 8527, 8835, 10081]\n",
    "\n",
    "Training set mean is 14855.115757575757\n",
    "Training set std is 6556.134705703313\n",
    "Anomaly indices in the test set are [2643 3884 5227 5535 6781]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n",
      "t_unit\n",
      "readings\n",
      "idx_anomaly\n",
      "idx_split\n",
      "training\n",
      "test\n",
      "train_m\n",
      "train_std\n",
      "t_train\n",
      "t_test\n",
      "idx_anomaly_test\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"../sample_data/nyc_taxi.npz\")\n",
    "for k in data.keys():\n",
    "    print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4820,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"test\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.532643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.551521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      value\n",
       "1  0.532643\n",
       "2  0.551521"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | export\n",
    "df = pd.DataFrame(data[\"test\"], index=data[\"t_test\"], columns=[\"value\"])\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 48  # so that one window is one day\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"subset\": get_window(\n",
    "            df.values,\n",
    "            window_size=p,\n",
    "            end_step=t,\n",
    "            indices=list(df.index),\n",
    "            return_indices=False,\n",
    "        ),\n",
    "        \"end_step\": t,\n",
    "        \"start_step\": t - p,\n",
    "    }\n",
    "    for t in range(\n",
    "        p, len(df), p\n",
    "    )  # take steps every 48 so that we end up with non-overlapping sequences\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Encoder(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 1), stride=(2, 1))\n",
       "    (conv2): Conv2d(32, 512, kernel_size=(3, 1), stride=(2, 1))\n",
       "    (linear): Linear(in_features=5632, out_features=512, bias=False)\n",
       "    (linear_mean): Linear(in_features=512, out_features=24, bias=False)\n",
       "    (linear_var): Linear(in_features=512, out_features=24, bias=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (linear): Linear(in_features=24, out_features=512, bias=False)\n",
       "    (dconv1): ConvTranspose2d(512, 128, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (dconv2): ConvTranspose2d(128, 64, kernel_size=(3, 1), stride=(1, 1))\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear_out): Linear(in_features=384, out_features=48, bias=False)\n",
       "  )\n",
       "  (latent_sampler): StochasticSampler()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_model_pth = f\"../models/vae_500_z24.pth\"\n",
    "vae_model = torch.load(vae_model_pth, map_location=device)\n",
    "vae_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_embeddings(x, n_windows=1, latent_dim=32, seq_len=1, sampler_repeat=200):\n",
    "    \"\"\"\n",
    "    _summary_\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : _type_\n",
    "        _description_\n",
    "    n_windows : int, optional\n",
    "        _description_, by default 1\n",
    "    latent_dim : int, optional\n",
    "        _description_, by default 32\n",
    "    seq_len : int, optional\n",
    "        _description_, by default 1\n",
    "    sampler_repeat : int, optional\n",
    "        Number of times to repeatedly sample from the sampler to ensure we have enough variablity in the embedding, by default 10\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _type_\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    # actual_shape = x.shape[0]\n",
    "    x = (\n",
    "        torch.from_numpy(x.astype(np.float32)).view(n_windows, -1, seq_len).to(device)\n",
    "    )  # p, seq -> n_windows, p, seq\n",
    "    embedded_x = torch.zeros(n_windows, latent_dim, seq_len).to(device)\n",
    "    for idx in range(n_windows):\n",
    "        batched_x_window = x[idx].unsqueeze(0)\n",
    "        # print(batched_x_window.shape)\n",
    "        z_mean, z_log_var = vae_model.encoder(batched_x_window)\n",
    "        for _ in range(sampler_repeat):\n",
    "            # explore multiple potential future embeddings by sampling from the latent space multiple times (Monte Carlo sampling).\n",
    "            embedded_x[idx] += (\n",
    "                vae_model.latent_sampler(z_mean, z_log_var).permute(1, 0)\n",
    "                / sampler_repeat\n",
    "            )\n",
    "\n",
    "        # reshape\n",
    "    embedded_x = embedded_x.reshape(latent_dim * n_windows, -1)\n",
    "    return embedded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1013],\n",
       "        [-0.1030],\n",
       "        [-0.0095],\n",
       "        [ 0.1046],\n",
       "        [ 0.0419],\n",
       "        [ 0.0247],\n",
       "        [-0.3435],\n",
       "        [ 0.1282],\n",
       "        [ 0.0182],\n",
       "        [-0.0849],\n",
       "        [-0.0338],\n",
       "        [ 1.7626],\n",
       "        [-0.0362],\n",
       "        [-0.0158],\n",
       "        [ 0.0415],\n",
       "        [-0.2683],\n",
       "        [ 0.4677],\n",
       "        [-0.0319],\n",
       "        [-0.0100],\n",
       "        [-0.0680],\n",
       "        [-0.0270],\n",
       "        [-0.0291],\n",
       "        [ 0.4637],\n",
       "        [ 0.0055]], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embeddings(data[0][\"subset\"], latent_dim=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embeddings = [\n",
    "    {\n",
    "        \"subset\": get_embeddings(\n",
    "            data[i][\"subset\"], latent_dim=24, n_windows=1, seq_len=1, sampler_repeat=200\n",
    "        ),\n",
    "        \"end_step\": data[i][\"end_step\"],\n",
    "        \"start_step\": data[i][\"start_step\"],\n",
    "    }\n",
    "    for i in range(len(data))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now the testing embeddings. Can have already one benchmark with VAE reconstruction. But we now pass this to the LSTM model to predict expected embedding for next window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ts_vae_lstm.lstm import LSTMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm_input): LSTM(24, 128, batch_first=True)\n",
       "  (lstm_hidden): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
       "  (lstm_output): LSTM(256, 24, batch_first=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model_pth = \"../models/lstm_200.pth\"\n",
    "lstm_model = torch.load(lstm_model_pth, map_location=device)\n",
    "lstm_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][\"subset\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 24, got 48",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lstm_model(torch\u001b[39m.\u001b[39;49mfrom_numpy(data[\u001b[39m0\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39msubset\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49mfloat32))\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n",
      "File \u001b[0;32m/mnt/data2/ts_vae-lstm/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/data2/ts_vae-lstm/ts_vae_lstm/lstm.py:100\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# bs, seq_len, emb_dim\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m# LSTM1\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m lstm_out1, (h1, c1) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm_input(x)\n\u001b[1;32m    101\u001b[0m lstm_out1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(lstm_out1))\n\u001b[1;32m    102\u001b[0m \u001b[39m# LSTM2\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data2/ts_vae-lstm/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/data2/ts_vae-lstm/.venv/lib/python3.9/site-packages/torch/nn/modules/rnn.py:810\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m--> 810\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m/mnt/data2/ts_vae-lstm/.venv/lib/python3.9/site-packages/torch/nn/modules/rnn.py:730\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    726\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[1;32m    727\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    728\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    729\u001b[0m                        ):\n\u001b[0;32m--> 730\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[1;32m    731\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    732\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[0] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    733\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    734\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/mnt/data2/ts_vae-lstm/.venv/lib/python3.9/site-packages/torch/nn/modules/rnn.py:218\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    215\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    216\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[1;32m    217\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    219\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    220\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 24, got 48"
     ]
    }
   ],
   "source": [
    "lstm_model(torch.from_numpy(data[0][\"subset\"].astype(np.float32)).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 24, got 48",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predicted_embeddings \u001b[39m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     {\n\u001b[1;32m      3\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msubset\u001b[39m\u001b[39m\"\u001b[39m: data[i][\u001b[39m\"\u001b[39m\u001b[39msubset\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      4\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnext_subset\u001b[39m\u001b[39m\"\u001b[39m: lstm_model(\n\u001b[1;32m      5\u001b[0m             torch\u001b[39m.\u001b[39mfrom_numpy(data[i][\u001b[39m\"\u001b[39m\u001b[39msubset\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m         )\n\u001b[1;32m      7\u001b[0m         \u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m         \u001b[39m.\u001b[39mcpu()\n\u001b[1;32m      9\u001b[0m         \u001b[39m.\u001b[39mnumpy(),\n\u001b[1;32m     10\u001b[0m     }\n\u001b[1;32m     11\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data_embeddings))\n\u001b[1;32m     12\u001b[0m ]\n",
      "Cell \u001b[0;32mIn[46], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m predicted_embeddings \u001b[39m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     {\n\u001b[1;32m      3\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msubset\u001b[39m\u001b[39m\"\u001b[39m: data[i][\u001b[39m\"\u001b[39m\u001b[39msubset\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m----> 4\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnext_subset\u001b[39m\u001b[39m\"\u001b[39m: lstm_model(\n\u001b[1;32m      5\u001b[0m             torch\u001b[39m.\u001b[39;49mfrom_numpy(data[i][\u001b[39m\"\u001b[39;49m\u001b[39msubset\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49mfloat32))\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m         )\n\u001b[1;32m      7\u001b[0m         \u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m         \u001b[39m.\u001b[39mcpu()\n\u001b[1;32m      9\u001b[0m         \u001b[39m.\u001b[39mnumpy(),\n\u001b[1;32m     10\u001b[0m     }\n\u001b[1;32m     11\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data_embeddings))\n\u001b[1;32m     12\u001b[0m ]\n",
      "File \u001b[0;32m/mnt/data2/ts_vae-lstm/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/data2/ts_vae-lstm/ts_vae_lstm/lstm.py:100\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# bs, seq_len, emb_dim\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m# LSTM1\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m lstm_out1, (h1, c1) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm_input(x)\n\u001b[1;32m    101\u001b[0m lstm_out1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(lstm_out1))\n\u001b[1;32m    102\u001b[0m \u001b[39m# LSTM2\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data2/ts_vae-lstm/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/data2/ts_vae-lstm/.venv/lib/python3.9/site-packages/torch/nn/modules/rnn.py:810\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m--> 810\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m/mnt/data2/ts_vae-lstm/.venv/lib/python3.9/site-packages/torch/nn/modules/rnn.py:730\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    726\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[1;32m    727\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    728\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    729\u001b[0m                        ):\n\u001b[0;32m--> 730\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[1;32m    731\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    732\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[0] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    733\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    734\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/mnt/data2/ts_vae-lstm/.venv/lib/python3.9/site-packages/torch/nn/modules/rnn.py:218\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    215\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    216\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[1;32m    217\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    219\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    220\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 24, got 48"
     ]
    }
   ],
   "source": [
    "predicted_embeddings = [\n",
    "    {\n",
    "        \"subset\": data[i][\"subset\"],\n",
    "        \"next_subset\": lstm_model(\n",
    "            torch.from_numpy(data[i][\"subset\"].astype(np.float32)).unsqueeze(0)\n",
    "        )\n",
    "        .view(-1, 1)\n",
    "        .cpu()\n",
    "        .numpy(),\n",
    "    }\n",
    "    for i in range(len(data_embeddings))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5500,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"training\"].shape\n",
    "with torch.no_grad():\n",
    "    xs_val, ys_val = next(iter(valid_dataloader))\n",
    "    pred_ys_val = model(xs_val.to(device))\n",
    "\n",
    "    for idx in range(batch_size):\n",
    "        if idx >= 8:\n",
    "            break\n",
    "        ax = plt.subplot(4, 2, idx + 1)\n",
    "        # idx_feature = 0\n",
    "        pred_current_example, current_points, prev_points = (\n",
    "            pred_ys_val[idx].detach().cpu().squeeze(),\n",
    "            ys_val[idx].detach().cpu().squeeze(),\n",
    "            xs_val[idx].detach().cpu().squeeze(),\n",
    "        )\n",
    "        pred_ts_prev = vae_model.decoder(prev_points.unsqueeze(0)).squeeze()\n",
    "        pred_ts_values = vae_model.decoder(pred_current_example.unsqueeze(0)).squeeze()\n",
    "        pred_ts_values_true = vae_model.decoder(current_points.unsqueeze(0)).squeeze()\n",
    "        sns.lineplot(\n",
    "            np.concatenate([pred_ts_prev.numpy(), pred_ts_values_true.numpy()], axis=0),\n",
    "            alpha=0.5,\n",
    "            linestyle=\"--\",\n",
    "        )  # , label=\"next true\")\n",
    "        padded_pred = np.concatenate(\n",
    "            [[None] * len(pred_ts_values), pred_ts_values.numpy()], axis=0\n",
    "        )\n",
    "        sns.lineplot(padded_pred)\n",
    "        mse_val = F.mse_loss(pred_ts_values, pred_ts_values_true)\n",
    "\n",
    "        plt.ylim(-2, 2)\n",
    "        plt.title(f\"MSE = {mse_val:.4f}\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.647416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.048144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.330880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.560078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.683378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5496</th>\n",
       "      <td>0.650634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5497</th>\n",
       "      <td>0.769805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5498</th>\n",
       "      <td>0.614499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5499</th>\n",
       "      <td>0.474090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5500</th>\n",
       "      <td>0.437365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5500 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         value\n",
       "1    -0.647416\n",
       "2    -1.048144\n",
       "3    -1.330880\n",
       "4    -1.560078\n",
       "5    -1.683378\n",
       "...        ...\n",
       "5496  0.650634\n",
       "5497  0.769805\n",
       "5498  0.614499\n",
       "5499  0.474090\n",
       "5500  0.437365\n",
       "\n",
       "[5500 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data[\"training\"], index=data[\"t_train\"], columns=[\"value\"])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae-lstm-kernel",
   "language": "python",
   "name": "vae-lstm-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
