{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE\n",
    "\n",
    "> Train a VAE to encode embeddings of normal behavior in the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a VAE with no anomalies in the time series.\n",
    "\n",
    "```--> Anomalies occur at:\n",
    "  timestamp #0: 2014-11-01 19:00:00\n",
    "  timestamp #1: 2014-11-27 15:30:00\n",
    "  timestamp #2: 2014-12-25 15:00:00\n",
    "  timestamp #3: 2015-01-01 01:00:00\n",
    "  timestamp #4: 2015-01-27 00:00:00\n",
    "\n",
    "Original csv file contains (10320,) timestamps.\n",
    "Processed time series contain (10320,) readings.\n",
    "Anomaly indices are [5943, 7184, 8527, 8835, 10081]\n",
    "\n",
    "Training set mean is 14855.115757575757\n",
    "Training set std is 6556.134705703313\n",
    "Anomaly indices in the test set are [2643 3884 5227 5535 6781]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# for configs\n",
    "from fastcore.xtras import Path\n",
    "import os\n",
    "from fastcore.xtras import partial\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# run only once\n",
    "\n",
    "try:\n",
    "    initialize(config_path=\"../config\", version_base=\"1.2\")\n",
    "    cfg = compose(config_name=\"config.yaml\")\n",
    "    cfg = OmegaConf.to_object(cfg)  # perform interpolation of the variables also\n",
    "    cfg = OmegaConf.create(cfg)  # so that dot-notation works?\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Got Exception while reading config:\\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEDIR = Path(cfg.base_dir).resolve()\n",
    "MODELDIR = Path(cfg.model_dir).resolve()\n",
    "DATAPATH = Path(cfg.dataset.path).resolve()\n",
    "\n",
    "print(f\"Base directory: {BASEDIR}\")\n",
    "print(f\"Model directory: {MODELDIR}\")\n",
    "print(f\"Dataset is {DATAPATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = cfg.num_workers if cfg.get(\"num_workers\", None) else os.cpu_count()\n",
    "print(f\"Number of workers: {num_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = cfg.device if cfg.device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime as dt\n",
    "from ts_vae_lstm.concepts import get_window\n",
    "from scipy import signal\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program expects a <font color='#ff0000'>univariate time series</font> in a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Reading {DATAPATH}\")\n",
    "if DATAPATH.name.endswith(\".npz\"):\n",
    "    data = np.load(DATAPATH)\n",
    "    print(data.keys())\n",
    "elif DATAPATH.name.endswith(\".csv\"):\n",
    "    data = pd.read_csv(DATAPATH)\n",
    "    print(data.columns)\n",
    "elif DATAPATH.name.endswith(\".parquet\"):\n",
    "    data = pd.read_parquet(DATAPATH)\n",
    "    print(data.columns)\n",
    "else:\n",
    "    raise NotImplementedError(\n",
    "        f\"Cannot read {DATAPATH}. Please raise an issue: thatgeeman/ts_vae-lstm\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean and standard deviations of training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMECOL = cfg.dataset.time_col\n",
    "SIGNALCOL = cfg.dataset.signal\n",
    "idx_split = cfg.dataset.idx_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the anomaly indices:\n",
    "- Used to validate the split of the data (thereby avoiding training on anomalous behaviour).\n",
    "- Also used for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_idxs = cfg.dataset.get(\"idx_anomaly\", None)\n",
    "if anomaly_idxs is not None:\n",
    "    assert (\n",
    "        anomaly_idxs[0] > idx_split\n",
    "    ), \"Expected training set to contain only non-anomalous data\"\n",
    "else:\n",
    "    print(\"No idx_anomaly set in config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the columns defined in the config file and make train-test splits. Independent of the file format, the indexing should normally work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = data[SIGNALCOL][:idx_split], data[SIGNALCOL][idx_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_m, train_std = data_train.mean(), data_train.std()\n",
    "train_m, train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the test dataset with the train mean and standard deviation.\n",
    "The train fold will still be split later, so not standardizing it now.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_norm = (data_test - train_m) / (train_std + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def as_df(data):\n",
    "    \"\"\"Returns data as dataframe with the column named `value`.\"\"\"\n",
    "    return pd.DataFrame(data=data, columns=[\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_df(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the windows and number of steps look fine visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_step = 0\n",
    "p = 48  # for one full day\n",
    "n_windows = 3\n",
    "sns.lineplot(\n",
    "    data=as_df(data_train[start_step : 1 + start_step + (p * n_windows)]).reset_index(),\n",
    "    x=\"index\",  # possible since we reset_index to add \"index\" column\n",
    "    y=\"value\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather data into structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = cfg.n_lag  # past sequences at time t. 48 steps = a day\n",
    "end_steps = [es for es in range(p, len(data_train), 1)]\n",
    "# step is one since we want overlapping windows for VAE training\n",
    "len(end_steps), end_steps[:3], end_steps[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_windowed = [\n",
    "    {\n",
    "        \"subset\": get_window(\n",
    "            data_train,\n",
    "            window_size=p,\n",
    "            end_step=t,\n",
    "            return_indices=False,\n",
    "        ),\n",
    "        \"end_step\": t,  # the time we want to predict for. For vae, we reconstruct the window.\n",
    "        \"start_step\": t - p,\n",
    "    }\n",
    "    for t in end_steps\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_windowed), data_windowed[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preseve 10% of the dataset from this for validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = cfg.test_split\n",
    "val_data_idxs = np.random.choice(\n",
    "    range(len(data_windowed)), size=int(split_ratio * len(data_windowed)), replace=False\n",
    ")\n",
    "trn_data_idxs = [idx for idx in range(len(data_windowed)) if idx not in val_data_idxs]\n",
    "len(val_data_idxs), len(trn_data_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = [data_windowed[idx] for idx in val_data_idxs]\n",
    "trn_data = [data_windowed[idx] for idx in trn_data_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate stats over training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data[0][\"subset\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = cfg.n_signals  # = 1, Since we have univariate time series.\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    n_features == trn_data[0][\"subset\"].shape[1]\n",
    "), f\"Got {n_features} in config but {trn_data[0]['subset'].shape[1]} features in data!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the train data mean and apply it to the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.zeros((len(trn_data), n_features))  # ((len(trn_data), 4))\n",
    "stds = np.zeros((len(trn_data), n_features))  # ((len(trn_data), 4))\n",
    "slice_from = n_features - 1\n",
    "for i, _trn_data in enumerate(trn_data):\n",
    "    means[i] = (np.mean(_trn_data[\"subset\"][:, slice_from:], axis=0)).astype(np.float32)\n",
    "    stds[i] = (np.var(_trn_data[\"subset\"][:, slice_from:], axis=0) ** 0.5).astype(\n",
    "        np.float32\n",
    "    )\n",
    "means = means.mean(0)\n",
    "stds = stds.mean(0)\n",
    "\n",
    "means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class TSDataset(Dataset):\n",
    "    def __init__(self, data, mean, std, slice_from=0):\n",
    "        self.data = data\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.slice_from = slice_from\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "        # ignore the timestamp column\n",
    "        x = self.data[idx][\"subset\"][:, self.slice_from :]  # 1024, 4\n",
    "        normed_X = ((x - self.mean) / (self.std + 1e-10)).astype(np.float32)\n",
    "        return torch.as_tensor(normed_X)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_trn = TSDataset(trn_data, mean=means, std=stds)\n",
    "dset_val = TSDataset(val_data, mean=means, std=stds)\n",
    "# use same stats from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_trn[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dset_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = cfg.batch_sz\n",
    "drop_last = cfg.drop_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_trn = DataLoader(\n",
    "    dataset=dset_trn,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=drop_last,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "dl_val = DataLoader(\n",
    "    dataset=dset_val,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=drop_last,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = next(iter(dl_trn))\n",
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_units = cfg.num_hidden_units\n",
    "kernel_size = cfg.kernel_size, n_features\n",
    "stride = cfg.stride, n_features\n",
    "latent_dim = cfg.latent_dim\n",
    "kernel_size, stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_c1 = nn.Conv2d(\n",
    "    in_channels=n_features,\n",
    "    out_channels=num_hidden_units // 8,\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    ")(xs.unsqueeze(1))\n",
    "xs_c2 = nn.Conv2d(\n",
    "    in_channels=num_hidden_units // 8,\n",
    "    out_channels=num_hidden_units,\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    ")(xs_c1)\n",
    "xs_c1.shape, xs_c2.shape  # , xs_c3.shape, xs_c4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        latent_dim=20,\n",
    "        num_hidden_units=512,\n",
    "        kernel_size=5,\n",
    "        stride=2,\n",
    "        act=F.mish,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=num_hidden_units // 8,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=num_hidden_units // 8,\n",
    "            out_channels=num_hidden_units,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden_units // 8)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden_units)\n",
    "\n",
    "        self.linear = nn.LazyLinear(\n",
    "            out_features=num_hidden_units,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.linear_mean = nn.LazyLinear(\n",
    "            out_features=latent_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.linear_var = nn.LazyLinear(\n",
    "            out_features=latent_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.act = act\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # 100, 1, 48, 1\n",
    "        x = self.act(self.bn1(self.conv1(x)))  # 100, 32, 23, 1\n",
    "        x = self.act(self.bn2(self.conv2(x)))  # 100, 64, 11, 1\n",
    "        x = self.flatten(x)  # 100, 512\n",
    "        x = self.act(self.linear(x))  # 100, 512\n",
    "        z_mean = self.linear_mean(x)\n",
    "        z_log_var = self.linear_var(x)\n",
    "        return z_mean, z_log_var\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.xavier_normal_(m.weight)\n",
    "                m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_mean, emb_var = Encoder(\n",
    "    in_channels=n_features,\n",
    "    latent_dim=latent_dim,\n",
    "    num_hidden_units=num_hidden_units,\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    ")(xs)\n",
    "emb_mean.shape, emb_var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_mean.mean(), emb_var.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Sampler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class StochasticSampler(nn.Module):\n",
    "    \"\"\"We basically want to parametrize the sampling from the latent space\"\"\"\n",
    "\n",
    "    def __init__(self, deterministic=False):\n",
    "        super().__init__()\n",
    "        self.sampler = torch.distributions.Normal(loc=0, scale=1)\n",
    "        self.deterministic = deterministic\n",
    "\n",
    "    def forward(self, z_mean, z_log_var):\n",
    "        \"\"\"Return a normal sample value Z from the latent space given a mean and variance\"\"\"\n",
    "        # z_mean and z_log_var are mean and log-var estimates of the latent space\n",
    "        # under the assumption that the latent space is a gaussian normal\n",
    "        device = z_mean.device\n",
    "        # Scales and shifts the sampled values using the reparameterization trick\n",
    "        eps = self.sampler.sample(z_mean.shape).squeeze().to(device)\n",
    "        # print(eps.shape, z_log_var.shape, z_mean.shape)\n",
    "        return (\n",
    "            z_mean\n",
    "            if self.deterministic\n",
    "            else (z_mean + torch.exp(0.5 * z_log_var) * eps)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = StochasticSampler(deterministic=False)\n",
    "emb = sampler(emb_mean, emb_var)\n",
    "emb.shape, emb.requires_grad, emb[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's called the reparameterization trick because it reparameterizes the sampling process into a deterministic function that can be used for backpropagation. The core idea is to introduce an epsilon term drawn from a standard normal distribution and combine it with the mean and log variance of the latent distribution in a specific way. This allows the model to learn the parameters of the latent distribution (mean and variance) while still maintaining the stochastic nature of sampling during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Decoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_c1 = nn.Linear(in_features=latent_dim, out_features=num_hidden_units, bias=False)(\n",
    "    emb\n",
    ")\n",
    "xs_c1 = xs_c1[:, :, None, None]\n",
    "xs_c2 = nn.ConvTranspose2d(\n",
    "    in_channels=num_hidden_units,\n",
    "    out_channels=num_hidden_units,\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    ")(xs_c1)\n",
    "xs_c3 = nn.ConvTranspose2d(\n",
    "    in_channels=num_hidden_units,\n",
    "    out_channels=num_hidden_units // 8,\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    ")(xs_c2)\n",
    "xs_c3 = nn.Flatten()(xs_c2)\n",
    "xs_c4 = nn.LazyLinear(out_features=p, bias=False)(xs_c3)\n",
    "(\n",
    "    xs_c1.shape,\n",
    "    xs_c2.shape,\n",
    "    xs_c3.shape,\n",
    "    xs_c4.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_features,\n",
    "        latent_dim=20,\n",
    "        num_hidden_units=512,\n",
    "        kernel_size=5,\n",
    "        stride=2,\n",
    "        act=F.mish,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert isinstance(\n",
    "            out_features, tuple\n",
    "        ), \"out_features must be a tuple of expected output shape\"\n",
    "        self.out_features = out_features\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=latent_dim, out_features=num_hidden_units, bias=False\n",
    "        )\n",
    "        self.dconv1 = nn.ConvTranspose2d(\n",
    "            in_channels=num_hidden_units,\n",
    "            out_channels=num_hidden_units,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "        )\n",
    "        self.dconv2 = nn.ConvTranspose2d(\n",
    "            in_channels=num_hidden_units,\n",
    "            out_channels=num_hidden_units // 8,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "        )\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden_units)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden_units // 8)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_out = nn.LazyLinear(\n",
    "            out_features=math.prod(out_features),\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.act = act\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x[:, :, None, None]\n",
    "        x = self.act(self.bn1(self.dconv1(x)))\n",
    "        x = self.act(self.bn2(self.dconv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        # no act for last layer\n",
    "        x = self.linear_out(x)\n",
    "        return self.reshape_to_output(x)\n",
    "\n",
    "    def reshape_to_output(self, x):\n",
    "        bs = x.shape[0]\n",
    "        return x.reshape(bs, *self.out_features)\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.xavier_normal_(m.weight)\n",
    "                m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decoder(\n",
    "    out_features=(p, n_features),\n",
    "    latent_dim=latent_dim,\n",
    "    num_hidden_units=num_hidden_units,\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    ")(emb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class VAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape,\n",
    "        latent_dim=20,\n",
    "        num_hidden_units=512,\n",
    "        kernel_size=5,\n",
    "        stride=2,\n",
    "        act=F.leaky_relu,\n",
    "        deterministic=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            in_channels=input_shape[1],\n",
    "            latent_dim=latent_dim,\n",
    "            act=act,\n",
    "            num_hidden_units=num_hidden_units,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            out_features=input_shape,\n",
    "            latent_dim=latent_dim,\n",
    "            act=act,\n",
    "            num_hidden_units=num_hidden_units,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "        )\n",
    "        self.latent_sampler = StochasticSampler(deterministic=deterministic)\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, sequence_length, num_features]\n",
    "        z_mean, z_log_var = self.encoder(x)\n",
    "        z = self.latent_sampler(z_mean, z_log_var)\n",
    "        reconstructed_x = self.decoder(z)\n",
    "        # loss to enforce all possible values are sampled from latent space\n",
    "        # should be of the size of the batch\n",
    "\n",
    "        # Reconstruction Loss (Mean Squared Error)\n",
    "        reconstruction_loss = F.mse_loss(reconstructed_x, x, reduction=\"mean\")\n",
    "        loss_kl = -0.5 * torch.mean(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
    "        # / x.size(1)\n",
    "        # average the KL divergence across the batch dimension\n",
    "        # In the VAE-LSTM paper, they mention normalizing the KL divergence term by the number of features in the input.\n",
    "        vae_loss = reconstruction_loss + loss_kl\n",
    "        return reconstructed_x, vae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(\n",
    "    input_shape=(p, n_features),\n",
    "    latent_dim=latent_dim,\n",
    "    num_hidden_units=num_hidden_units,\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    ")\n",
    "xs_pred, loss = model(xs)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_pred.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def validate_epoch(model, dls, scorer, device=\"cpu\"):\n",
    "    \"\"\"For the full dataloader, calculate the running loss and score\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_score = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, xs in enumerate(dls):\n",
    "            # move to device\n",
    "            xs = xs.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            xs_gen, loss = model(xs)\n",
    "            # calc score\n",
    "            score = scorer(xs, xs_gen)[\"mse\"]\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_score += score\n",
    "    return running_loss / len(dls), running_score / len(dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_reconstruction(original_signal, reconstructed_signal):\n",
    "    \"\"\"\n",
    "    Evaluates the quality of the reconstructed signal compared to the original signal.\n",
    "\n",
    "    Args:\n",
    "        original_signal (torch.Tensor): Original time series signal.\n",
    "        reconstructed_signal (torch.Tensor): Reconstructed signal from the VAE.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the evaluation metrics:\n",
    "            - mse: Mean Squared Error\n",
    "            - mae: Mean Absolute Error\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure tensors are on the CPU and numpy arrays\n",
    "    original_signal = original_signal.cpu().numpy()\n",
    "    reconstructed_signal = reconstructed_signal.cpu().numpy()\n",
    "\n",
    "    # Mean Squared Error\n",
    "    mse = nn.MSELoss()(\n",
    "        torch.from_numpy(original_signal), torch.from_numpy(reconstructed_signal)\n",
    "    ).item()\n",
    "\n",
    "    # Mean Absolute Error\n",
    "    mae = nn.L1Loss()(\n",
    "        torch.from_numpy(original_signal), torch.from_numpy(reconstructed_signal)\n",
    "    ).item()\n",
    "\n",
    "    return {\n",
    "        \"mse\": mse,\n",
    "        \"mae\": mae,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "batch_size = cfg.batch_sz\n",
    "# range_of_vals = 3.5  # output values will be in the range -3 to 3\n",
    "activation = F.mish\n",
    "latent_dim = latent_dim  # int(p * n_features // 2)\n",
    "learning_rate = cfg.lr\n",
    "num_epochs = cfg.epochs\n",
    "\n",
    "dset_trn = TSDataset(trn_data, mean=means, std=stds)\n",
    "dset_val = TSDataset(val_data, mean=means, std=stds)\n",
    "dl_trn = DataLoader(\n",
    "    dataset=dset_trn,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    shuffle=train,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "dl_val = DataLoader(\n",
    "    dataset=dset_val,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "#\n",
    "train_dataloader = dl_trn\n",
    "valid_dataloader = dl_val\n",
    "print_every = len(train_dataloader)  # epoch number to print at\n",
    "\n",
    "print(f\"latent_dim is {latent_dim}\")\n",
    "model = VAE(\n",
    "    latent_dim=latent_dim,\n",
    "    input_shape=(p, n_features),\n",
    "    act=activation,\n",
    "    num_hidden_units=num_hidden_units,\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    ").to(device)  # to make visualization easier, 2 latent dims\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=cfg.wd)\n",
    "scorer = evaluate_reconstruction  # calculate_smape\n",
    "# Training loop\n",
    "\n",
    "# Define LR scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=cfg.factor, patience=cfg.patience, min_lr=1e-8, verbose=True\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_score = 0.0\n",
    "    for batch_idx, xs in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # move to device\n",
    "        xs = xs.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        xs_gen, loss = model(xs)\n",
    "        # calc score\n",
    "        score = scorer(xs_gen, xs)[\"mse\"]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_score += score\n",
    "\n",
    "    if (batch_idx + 1) % print_every == 0:\n",
    "        # calculate loss for valid_dataloader\n",
    "        n_dls = len(train_dataloader)\n",
    "        val_loss, val_score = validate_epoch(\n",
    "            model, valid_dataloader, scorer=scorer, device=device\n",
    "        )\n",
    "        print(\n",
    "            \"Epoch [{}/{}], Batch [{}/{}], Loss: [{:.3f}, {:.3f}], Score: [{:.3f}, {:.3f}]\".format(\n",
    "                epoch + 1,\n",
    "                num_epochs,\n",
    "                batch_idx + 1,\n",
    "                n_dls,\n",
    "                running_loss / n_dls,\n",
    "                val_loss,\n",
    "                running_score / n_dls,\n",
    "                val_score,\n",
    "            )\n",
    "        )\n",
    "    # reset at end of epoch\n",
    "    running_loss = 0.0\n",
    "    running_score = 0.0\n",
    "    # Step the LR scheduler\n",
    "    scheduler.step(val_loss)  # min the running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_slug = int(time.time())\n",
    "time_slug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pth = f\"{MODELDIR}/vae_{num_epochs}_z{latent_dim}_{time_slug}.pth\"\n",
    "model_pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to path\n",
    "torch.save(model, model_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model_pth)\n",
    "with torch.no_grad():\n",
    "    xs_val = next(iter(dl_val))\n",
    "    xs_val_gen, loss_kl = model(xs_val.to(device))\n",
    "\n",
    "for idx in range(batch_size):\n",
    "    if idx >= 10:\n",
    "        break\n",
    "    plt.subplot(5, 2, idx + 1)\n",
    "    idx_feature = 0\n",
    "    decoded_example, actual_example = (\n",
    "        xs_val_gen[idx].detach().cpu(),\n",
    "        xs_val[idx].detach().cpu(),\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        actual_example[:, idx_feature].numpy(),\n",
    "        alpha=0.5,\n",
    "        label=\"true\",\n",
    "    )\n",
    "    sns.lineplot(decoded_example[:, idx_feature].numpy())  # , label=\"vae\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    xs_val = next(iter(dl_val))\n",
    "    emb_mean_val, emb_std_val = model.encoder(xs_val.to(device))\n",
    "    emb_val = model.latent_sampler(emb_mean_val, emb_std_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_val.cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=emb_val[:, 0].cpu(), y=emb_val[:, 2].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts_vae-lstm",
   "language": "python",
   "name": "ts_vae-lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
