# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_vae.ipynb.

# %% auto 0
__all__ = ['num_workers', 'BASEDIR', 'MODELDIR', 'data', 'df', 'p', 'end_steps', 'data_windowed', 'split_ratio', 'val_data_idxs',
           'trn_data_idxs', 'val_data', 'trn_data', 'n_features', 'means', 'stds', 'slice_from', 'dset_trn', 'dset_val',
           'batch_size', 'dl_trn', 'dl_val', 'device', 'TSDataset', 'Encoder', 'StochasticSampler', 'Decoder', 'VAE',
           'evaluate_reconstruction', 'validate_epoch']

# %% ../nbs/01_vae.ipynb 3
from dotenv import load_dotenv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime as dt
from .concepts import get_window
from scipy import signal
import os
import math

# %% ../nbs/01_vae.ipynb 4
num_workers = os.cpu_count()
num_workers

# %% ../nbs/01_vae.ipynb 6
load_dotenv()

BASEDIR = os.getenv("BASEDIR")
MODELDIR = os.getenv("MODELDIR")

# %% ../nbs/01_vae.ipynb 7
data = np.load(f"{BASEDIR}/sample_data/nyc_taxi.npz")
for k in data.keys():
    print(k)

# %% ../nbs/01_vae.ipynb 15
df = pd.DataFrame(data["training"], index=data["t_train"], columns=["value"])
df.head(2)

# %% ../nbs/01_vae.ipynb 20
p = 48  # past sequences at time t. 48 steps = a day
end_steps = [es for es in range(p, len(df), 1)]
# step is one since we want overlapping windows for VAE training
len(end_steps), end_steps[:3], end_steps[-3:]

# %% ../nbs/01_vae.ipynb 21
data_windowed = [
    {
        "subset": get_window(
            df.values,
            window_size=p,
            end_step=t,
            indices=list(df.index),
            return_indices=False,
        ),
        "end_step": t,  # the time we want to predict for. For vae, we reconstruct the window.
        "start_step": t - p,
    }
    for t in end_steps
]

# %% ../nbs/01_vae.ipynb 24
split_ratio = 0.2
val_data_idxs = np.random.choice(
    range(len(data_windowed)), size=int(split_ratio * len(data_windowed)), replace=False
)
trn_data_idxs = [idx for idx in range(len(data_windowed)) if idx not in val_data_idxs]
len(val_data_idxs), len(trn_data_idxs)

# %% ../nbs/01_vae.ipynb 25
from torch import nn
import torch.nn.functional as F
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import roc_auc_score, accuracy_score

# %% ../nbs/01_vae.ipynb 26
val_data = [data_windowed[idx] for idx in val_data_idxs]
trn_data = [data_windowed[idx] for idx in trn_data_idxs]

# %% ../nbs/01_vae.ipynb 29
n_features = trn_data[0]["subset"].shape[1]  # - 1
n_features

# %% ../nbs/01_vae.ipynb 31
means = np.zeros((len(trn_data), n_features))  # ((len(trn_data), 4))
stds = np.zeros((len(trn_data), n_features))  # ((len(trn_data), 4))
slice_from = n_features - 1
"""
for i, _trn_data in enumerate(trn_data):
    means[i] = (np.mean(_trn_data["subset"][:, slice_from:], axis=0)).astype(np.float32)
    stds[i] = (np.var(_trn_data["subset"][:, slice_from:], axis=0) ** 0.5).astype(
        np.float32
    )
"""
means = means.mean(0)
stds = stds.mean(0)

means, stds

# %% ../nbs/01_vae.ipynb 34
class TSDataset(Dataset):
    def __init__(self, data, mean, std):
        self.data = data
        self.mean = mean
        self.std = std

    def __getitem__(self, idx):
        # output[channel] = (input[channel] - mean[channel]) / std[channel]
        # ignore the timestamp column
        x = self.data[idx]["subset"][:, slice_from:]  # 1024, 4
        normed_X = ((x - self.mean) / (self.std + 1e-8)).astype(np.float32)
        return torch.as_tensor(normed_X)

    def __len__(self):
        return len(self.data)

# %% ../nbs/01_vae.ipynb 35
dset_trn = TSDataset(trn_data, mean=means, std=stds)
dset_val = TSDataset(val_data, mean=means, std=stds)
# use same stats from training data

# %% ../nbs/01_vae.ipynb 38
batch_size = 8

# %% ../nbs/01_vae.ipynb 39
dl_trn = DataLoader(
    dataset=dset_trn,
    batch_size=batch_size,
    drop_last=True,
    shuffle=True,
    num_workers=num_workers,
)
dl_val = DataLoader(
    dataset=dset_val,
    batch_size=batch_size,
    drop_last=True,
    shuffle=False,
    num_workers=num_workers,
)

# %% ../nbs/01_vae.ipynb 45
# encoder
# l_win to 24, the model would consider each 24-hour period as one sequence.
# pad: if your array is [1, 2, 3] and you symmetrically pad it with 1 unit, the result would be [2, 1, 2, 3, 2].
# xavier_initializer()
# conv 1: num_hidden_units / 16
# conv 2: num_hidden_units / 8
# conv 3: num_hidden_units / 4
# conv 4: num_hidden_units / 1, kernel = 4, 1
# padding : same


class Encoder(nn.Module):
    def __init__(
        self,
        latent_dim=20,
        num_hidden_units=512,
        kernel_size=(5, 1),
        stride=(2, 1),
        act=F.mish,
    ):
        super().__init__()
        self.flatten = nn.Flatten()
        self.conv1 = nn.Conv2d(
            in_channels=1,
            out_channels=num_hidden_units // 8,
            kernel_size=kernel_size,
            stride=stride,
        )
        self.conv2 = nn.Conv2d(
            in_channels=num_hidden_units // 8,
            out_channels=num_hidden_units,
            kernel_size=kernel_size,
            stride=stride,
        )
        self.bn1 = nn.BatchNorm2d(num_hidden_units // 8)
        self.bn2 = nn.BatchNorm2d(num_hidden_units)

        self.linear = nn.LazyLinear(
            out_features=num_hidden_units,
            bias=False,
        )
        self.linear_mean = nn.LazyLinear(
            out_features=latent_dim,
            bias=False,
        )
        self.linear_var = nn.LazyLinear(
            out_features=latent_dim,
            bias=False,
        )
        self.act = act
        self.init_weights()

    def forward(self, x):
        x = x.unsqueeze(1)  # 100, 1, 48, 1
        x = self.act(self.bn1(self.conv1(x)))  # 100, 32, 23, 1
        x = self.act(self.bn2(self.conv2(x)))  # 100, 64, 11, 1
        x = self.flatten(x)  # 100, 512
        x = self.act(self.linear(x))  # 100, 512
        z_mean = self.linear_mean(x)
        z_log_var = self.linear_var(x)
        return z_mean, z_log_var

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                torch.nn.init.xavier_normal_(m.weight)
                m.bias.data.fill_(0)

# %% ../nbs/01_vae.ipynb 49
class StochasticSampler(nn.Module):
    """We basically want to parametrize the sampling from the latent space"""

    def __init__(self, deterministic=False):
        super().__init__()
        self.sampler = torch.distributions.Normal(loc=0, scale=1)
        self.deterministic = deterministic

    def forward(self, z_mean, z_log_var):
        """Return a normal sample value Z from the latent space given a mean and variance"""
        # z_mean and z_log_var are mean and log-var estimates of the latent space
        # under the assumption that the latent space is a gaussian normal
        device = z_mean.device
        # Scales and shifts the sampled values using the reparameterization trick
        eps = self.sampler.sample(z_mean.shape).squeeze().to(device)
        # print(eps.shape, z_log_var.shape, z_mean.shape)
        return (
            z_mean
            if self.deterministic
            else (z_mean + torch.exp(0.5 * z_log_var) * eps)
        )

# %% ../nbs/01_vae.ipynb 55
# l_win to 24, the model would consider each 24-hour period as one sequence.
# pad: if your array is [1, 2, 3] and you symmetrically pad it with 1 unit, the result would be [2, 1, 2, 3, 2].
# xavier_initializer()
# dense 1: num_hidden_units
# reshape: (bs, 1, 1, num_hidden_units)  -> this is tensorflow notation, channel at end so actually (bs, num_hidden_units, 1, 1)

# conv 2: num_hidden_units, kernel = 1
# reshape: (bs, 4, 1, num_hidden_units / 4)

# conv 3: num_hidden_units / 4, kernel = 3, 1, stride = 1
# permute depth to spatial tf
# reshape: (bs, 8, 1, num_hidden_units / 8),

# conv 4: num_hidden_units / 8,  kernel = 3, 1, stride = 1
# permute depth to spatial tf
# reshape: (bs, 16, 1, num_hidden_units / 16)

# conv 5: num_hidden_units / 16, kernel = 3, 1, stride = 1
# permute depth to spatial tf
# reshape: (bs, num_hidden_units /16, 1,  16)

# conv 6: num_channel, kernel = 9, 1, stride = 1
# reshape: (bs, l_win, num_channel)


class Decoder(nn.Module):
    def __init__(
        self,
        output_shape,
        latent_dim=20,
        num_hidden_units=512,
        kernel_size=(5, 1),
        stride=2,
        act=F.mish,
    ):
        super().__init__()
        self.output_shape = output_shape
        self.linear = nn.Linear(
            in_features=latent_dim, out_features=num_hidden_units, bias=False
        )
        self.dconv1 = nn.ConvTranspose2d(
            in_channels=num_hidden_units,
            out_channels=num_hidden_units,
            kernel_size=kernel_size,
            stride=stride,
        )
        self.dconv2 = nn.ConvTranspose2d(
            in_channels=num_hidden_units,
            out_channels=num_hidden_units // 8,
            kernel_size=kernel_size,
            stride=stride,
        )

        self.bn1 = nn.BatchNorm2d(num_hidden_units)
        self.bn2 = nn.BatchNorm2d(num_hidden_units // 8)

        self.flatten = nn.Flatten()
        self.linear_out = nn.LazyLinear(
            out_features=math.prod(output_shape),
            bias=False,
        )

        self.act = act

        self.init_weights()

    def forward(self, x):
        x = self.linear(x)
        x = x[:, :, None, None]
        x = self.act(self.bn1(self.dconv1(x)))
        x = self.act(self.bn2(self.dconv2(x)))
        x = self.flatten(x)
        # no act for last layer
        x = self.linear_out(x)
        return self.reshape_to_output(x)

    def reshape_to_output(self, x):
        bs = x.shape[0]
        return x.reshape(bs, *self.output_shape)

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                torch.nn.init.xavier_normal_(m.weight)
                m.bias.data.fill_(0)

# %% ../nbs/01_vae.ipynb 59
class VAE(nn.Module):
    def __init__(
        self, input_shape, latent_dim=20, act=F.leaky_relu, deterministic=False
    ):
        super().__init__()
        self.encoder = Encoder(latent_dim=latent_dim, act=act)
        self.decoder = Decoder(output_shape=input_shape, latent_dim=latent_dim, act=act)
        self.latent_sampler = StochasticSampler(deterministic=deterministic)
        self.act = act

    def forward(self, x):
        # x shape: [batch_size, sequence_length, num_features]
        z_mean, z_log_var = self.encoder(x)
        z = self.latent_sampler(z_mean, z_log_var)
        reconstructed_x = self.decoder(z)
        # loss to enforce all possible values are sampled from latent space
        # should be of the size of the batch

        # Reconstruction Loss (Mean Squared Error)
        reconstruction_loss = F.mse_loss(reconstructed_x, x, reduction="mean")
        loss_kl = -0.5 * torch.mean(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())
        # / x.size(1)
        # average the KL divergence across the batch dimension
        # In the VAE-LSTM paper, they mention normalizing the KL divergence term by the number of features in the input.
        vae_loss = reconstruction_loss + loss_kl
        return reconstructed_x, vae_loss

# %% ../nbs/01_vae.ipynb 64
@torch.no_grad()
def evaluate_reconstruction(original_signal, reconstructed_signal):
    """
    Evaluates the quality of the reconstructed signal compared to the original signal.

    Args:
        original_signal (torch.Tensor): Original time series signal.
        reconstructed_signal (torch.Tensor): Reconstructed signal from the VAE.

    Returns:
        dict: Dictionary containing the evaluation metrics:
            - mse: Mean Squared Error
            - mae: Mean Absolute Error
            - dtw: Dynamic Time Warping distance
    """

    # Ensure tensors are on the CPU and numpy arrays
    original_signal = original_signal.cpu().numpy()
    reconstructed_signal = reconstructed_signal.cpu().numpy()

    # Mean Squared Error
    mse = nn.MSELoss()(
        torch.from_numpy(original_signal), torch.from_numpy(reconstructed_signal)
    ).item()

    # Mean Absolute Error
    mae = nn.L1Loss()(
        torch.from_numpy(original_signal), torch.from_numpy(reconstructed_signal)
    ).item()

    return {
        "mse": mse,
        "mae": mae,
    }

# %% ../nbs/01_vae.ipynb 66
device = "cuda" if torch.cuda.is_available() else "cpu"
device

# %% ../nbs/01_vae.ipynb 67
def validate_epoch(dls, scorer):
    """For the full dataloader, calculate the running loss and score"""
    model.eval()
    running_loss = 0.0
    running_score = 0.0
    with torch.no_grad():
        for batch_idx, xs in enumerate(dls):
            # move to device
            xs = xs.to(device)

            # Forward pass
            xs_gen, loss = model(xs)
            # calc score
            score = scorer(xs, xs_gen)["mse"]

            running_loss += loss.item()
            running_score += score
    return running_loss / len(dls), running_score / len(dls)

# %% ../nbs/01_vae.ipynb 68
from fastcore.xtras import partial
