"""Train a VAE to encode embeddings of normal behavior in the time series."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_vae.ipynb.

# %% auto 0
__all__ = ['BASEDIR', 'MODELDIR', 'DATAPATH', 'num_workers', 'device', 'TIMECOL', 'SIGNALCOL', 'idx_split', 'anomaly_idxs',
           'data_train', 'data_test', 'train_m', 'train_std', 'data_test_norm', 'p', 'end_steps', 'data_windowed',
           'split_ratio', 'val_data_idxs', 'trn_data_idxs', 'val_data', 'trn_data', 'n_features', 'means', 'stds',
           'slice_from', 'dset_trn', 'dset_val', 'batch_size', 'drop_last', 'dl_trn', 'dl_val', 'as_df', 'TSDataset',
           'Encoder', 'StochasticSampler', 'Decoder', 'VAE', 'evaluate_reconstruction', 'validate_epoch']

# %% ../nbs/01_vae.ipynb 3
# for configs
from hydra import compose, initialize
from omegaconf import OmegaConf
from fastcore.xtras import Path
import os

# %% ../nbs/01_vae.ipynb 5
BASEDIR = Path(cfg.base_dir).resolve()
MODELDIR = Path(cfg.model_dir).resolve()
DATAPATH = Path(cfg.dataset.path).resolve()

print(f"Base directory: {BASEDIR}")
print(f"Model directory: {MODELDIR}")
print(f"Dataset is {DATAPATH}")

# %% ../nbs/01_vae.ipynb 6
num_workers = cfg.num_workers if cfg.get("num_workers", None) else os.cpu_count()
print(f"Number of workers: {num_workers}")

# %% ../nbs/01_vae.ipynb 7
device = cfg.device if cfg.device else ("cuda" if torch.cuda.is_available() else "cpu")
device

# %% ../nbs/01_vae.ipynb 9
from dotenv import load_dotenv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime as dt
from .concepts import get_window
from scipy import signal
import math

# %% ../nbs/01_vae.ipynb 11
print(f"Reading {DATAPATH}")
if DATAPATH.name.endswith(".npz"):
    data = np.load(DATAPATH)
    print(data.keys())
elif DATAPATH.name.endswith(".csv"):
    data = pd.read_csv(DATAPATH)
    print(data.columns)
elif DATAPATH.name.endswith(".parquet"):
    data = pd.read_parquet(DATAPATH)
    print(data.columns)
else:
    raise NotImplementedError(
        f"Cannot read {DATAPATH}. Please raise an issue: thatgeeman/ts_vae-lstm"
    )

# %% ../nbs/01_vae.ipynb 13
TIMECOL = cfg.dataset.time_col
SIGNALCOL = cfg.dataset.signal
idx_split = cfg.dataset.idx_split

# %% ../nbs/01_vae.ipynb 15
anomaly_idxs = cfg.dataset.get("idx_anomaly", None)
if anomaly_idxs is not None:
    assert (
        anomaly_idxs[0] > idx_split
    ), "Expected training set to contain only non-anomalous data"
else:
    print("No idx_anomaly set in config")

# %% ../nbs/01_vae.ipynb 17
data_train, data_test = data[SIGNALCOL][:idx_split], data[SIGNALCOL][idx_split:]

# %% ../nbs/01_vae.ipynb 18
data_train.shape, data_test.shape

# %% ../nbs/01_vae.ipynb 19
train_m, train_std = data_train.mean(), data_train.std()
train_m, train_std

# %% ../nbs/01_vae.ipynb 21
data_test_norm = (data_test - train_m) / (train_std + 1e-10)

# %% ../nbs/01_vae.ipynb 22
def as_df(data):
    """Returns data as dataframe with the column named `value`."""
    return pd.DataFrame(data=data, columns=["value"])

# %% ../nbs/01_vae.ipynb 27
p = cfg.n_lag  # past sequences at time t. 48 steps = a day
end_steps = [es for es in range(p, len(data_train), 1)]
# step is one since we want overlapping windows for VAE training
len(end_steps), end_steps[:3], end_steps[-3:]

# %% ../nbs/01_vae.ipynb 28
data_windowed = [
    {
        "subset": get_window(
            data_train,
            window_size=p,
            end_step=t,
            return_indices=False,
        ),
        "end_step": t,  # the time we want to predict for. For vae, we reconstruct the window.
        "start_step": t - p,
    }
    for t in end_steps
]

# %% ../nbs/01_vae.ipynb 31
split_ratio = cfg.test_split
val_data_idxs = np.random.choice(
    range(len(data_windowed)), size=int(split_ratio * len(data_windowed)), replace=False
)
trn_data_idxs = [idx for idx in range(len(data_windowed)) if idx not in val_data_idxs]
len(val_data_idxs), len(trn_data_idxs)

# %% ../nbs/01_vae.ipynb 32
from torch import nn
import torch.nn.functional as F
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import roc_auc_score, accuracy_score

# %% ../nbs/01_vae.ipynb 33
val_data = [data_windowed[idx] for idx in val_data_idxs]
trn_data = [data_windowed[idx] for idx in trn_data_idxs]

# %% ../nbs/01_vae.ipynb 37
n_features = cfg.n_signals  # = 1, Since we have univariate time series.
n_features

# %% ../nbs/01_vae.ipynb 38
assert (
    n_features == trn_data[0]["subset"].shape[1]
), f"Got {n_features} in config but {trn_data[0]['subset'].shape[1]} features in data!"

# %% ../nbs/01_vae.ipynb 40
means = np.zeros((len(trn_data), n_features))  # ((len(trn_data), 4))
stds = np.zeros((len(trn_data), n_features))  # ((len(trn_data), 4))
slice_from = n_features - 1
for i, _trn_data in enumerate(trn_data):
    means[i] = (np.mean(_trn_data["subset"][:, slice_from:], axis=0)).astype(np.float32)
    stds[i] = (np.var(_trn_data["subset"][:, slice_from:], axis=0) ** 0.5).astype(
        np.float32
    )
means = means.mean(0)
stds = stds.mean(0)

means, stds

# %% ../nbs/01_vae.ipynb 41
class TSDataset(Dataset):
    def __init__(self, data, mean, std):
        self.data = data
        self.mean = mean
        self.std = std

    def __getitem__(self, idx):
        # output[channel] = (input[channel] - mean[channel]) / std[channel]
        # ignore the timestamp column
        x = self.data[idx]["subset"][:, slice_from:]  # 1024, 4
        normed_X = ((x - self.mean) / (self.std + 1e-10)).astype(np.float32)
        return torch.as_tensor(normed_X)

    def __len__(self):
        return len(self.data)

# %% ../nbs/01_vae.ipynb 42
dset_trn = TSDataset(trn_data, mean=means, std=stds)
dset_val = TSDataset(val_data, mean=means, std=stds)
# use same stats from training data

# %% ../nbs/01_vae.ipynb 45
batch_size = cfg.batch_sz
drop_last = cfg.drop_last

# %% ../nbs/01_vae.ipynb 46
dl_trn = DataLoader(
    dataset=dset_trn,
    batch_size=batch_size,
    drop_last=drop_last,
    shuffle=True,
    num_workers=num_workers,
)
dl_val = DataLoader(
    dataset=dset_val,
    batch_size=batch_size,
    drop_last=drop_last,
    shuffle=False,
    num_workers=num_workers,
)

# %% ../nbs/01_vae.ipynb 53
# encoder
# l_win to 24, the model would consider each 24-hour period as one sequence.
# pad: if your array is [1, 2, 3] and you symmetrically pad it with 1 unit, the result would be [2, 1, 2, 3, 2].
# xavier_initializer()
# conv 1: num_hidden_units / 16
# conv 2: num_hidden_units / 8
# conv 3: num_hidden_units / 4
# conv 4: num_hidden_units / 1, kernel = 4, 1
# padding : same


class Encoder(nn.Module):
    def __init__(
        self,
        latent_dim=20,
        num_hidden_units=512,
        kernel_size=5,
        stride=2,
        act=F.mish,
    ):
        super().__init__()
        self.flatten = nn.Flatten()
        self.conv1 = nn.Conv2d(
            in_channels=n_features,
            out_channels=num_hidden_units // 8,
            kernel_size=kernel_size,
            stride=stride,
        )
        self.conv2 = nn.Conv2d(
            in_channels=num_hidden_units // 8,
            out_channels=num_hidden_units,
            kernel_size=kernel_size,
            stride=stride,
        )
        self.bn1 = nn.BatchNorm2d(num_hidden_units // 8)
        self.bn2 = nn.BatchNorm2d(num_hidden_units)

        self.linear = nn.LazyLinear(
            out_features=num_hidden_units,
            bias=False,
        )
        self.linear_mean = nn.LazyLinear(
            out_features=latent_dim,
            bias=False,
        )
        self.linear_var = nn.LazyLinear(
            out_features=latent_dim,
            bias=False,
        )
        self.act = act
        self.init_weights()

    def forward(self, x):
        x = x.unsqueeze(1)  # 100, 1, 48, 1
        x = self.act(self.bn1(self.conv1(x)))  # 100, 32, 23, 1
        x = self.act(self.bn2(self.conv2(x)))  # 100, 64, 11, 1
        x = self.flatten(x)  # 100, 512
        x = self.act(self.linear(x))  # 100, 512
        z_mean = self.linear_mean(x)
        z_log_var = self.linear_var(x)
        return z_mean, z_log_var

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                torch.nn.init.xavier_normal_(m.weight)
                m.bias.data.fill_(0)

# %% ../nbs/01_vae.ipynb 58
class StochasticSampler(nn.Module):
    """We basically want to parametrize the sampling from the latent space"""

    def __init__(self, deterministic=False):
        super().__init__()
        self.sampler = torch.distributions.Normal(loc=0, scale=1)
        self.deterministic = deterministic

    def forward(self, z_mean, z_log_var):
        """Return a normal sample value Z from the latent space given a mean and variance"""
        # z_mean and z_log_var are mean and log-var estimates of the latent space
        # under the assumption that the latent space is a gaussian normal
        device = z_mean.device
        # Scales and shifts the sampled values using the reparameterization trick
        eps = self.sampler.sample(z_mean.shape).squeeze().to(device)
        # print(eps.shape, z_log_var.shape, z_mean.shape)
        return (
            z_mean
            if self.deterministic
            else (z_mean + torch.exp(0.5 * z_log_var) * eps)
        )

# %% ../nbs/01_vae.ipynb 64
# l_win to 24, the model would consider each 24-hour period as one sequence.
# pad: if your array is [1, 2, 3] and you symmetrically pad it with 1 unit, the result would be [2, 1, 2, 3, 2].
# xavier_initializer()
# dense 1: num_hidden_units
# reshape: (bs, 1, 1, num_hidden_units)  -> this is tensorflow notation, channel at end so actually (bs, num_hidden_units, 1, 1)

# conv 2: num_hidden_units, kernel = 1
# reshape: (bs, 4, 1, num_hidden_units / 4)

# conv 3: num_hidden_units / 4, kernel = 3, 1, stride = 1
# permute depth to spatial tf
# reshape: (bs, 8, 1, num_hidden_units / 8),

# conv 4: num_hidden_units / 8,  kernel = 3, 1, stride = 1
# permute depth to spatial tf
# reshape: (bs, 16, 1, num_hidden_units / 16)

# conv 5: num_hidden_units / 16, kernel = 3, 1, stride = 1
# permute depth to spatial tf
# reshape: (bs, num_hidden_units /16, 1,  16)

# conv 6: num_channel, kernel = 9, 1, stride = 1
# reshape: (bs, l_win, num_channel)


class Decoder(nn.Module):
    def __init__(
        self,
        output_shape,
        latent_dim=20,
        num_hidden_units=512,
        kernel_size=5,
        stride=2,
        act=F.mish,
    ):
        super().__init__()
        self.output_shape = output_shape
        self.linear = nn.Linear(
            in_features=latent_dim, out_features=num_hidden_units, bias=False
        )
        self.dconv1 = nn.ConvTranspose2d(
            in_channels=num_hidden_units,
            out_channels=num_hidden_units,
            kernel_size=kernel_size,
            stride=stride,
        )
        self.dconv2 = nn.ConvTranspose2d(
            in_channels=num_hidden_units,
            out_channels=num_hidden_units // 8,
            kernel_size=kernel_size,
            stride=stride,
        )

        self.bn1 = nn.BatchNorm2d(num_hidden_units)
        self.bn2 = nn.BatchNorm2d(num_hidden_units // 8)

        self.flatten = nn.Flatten()
        self.linear_out = nn.LazyLinear(
            out_features=math.prod(output_shape),
            bias=False,
        )

        self.act = act

        self.init_weights()

    def forward(self, x):
        x = self.linear(x)
        x = x[:, :, None, None]
        x = self.act(self.bn1(self.dconv1(x)))
        x = self.act(self.bn2(self.dconv2(x)))
        x = self.flatten(x)
        # no act for last layer
        x = self.linear_out(x)
        return self.reshape_to_output(x)

    def reshape_to_output(self, x):
        bs = x.shape[0]
        return x.reshape(bs, *self.output_shape)

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                torch.nn.init.xavier_normal_(m.weight)
                m.bias.data.fill_(0)

# %% ../nbs/01_vae.ipynb 68
class VAE(nn.Module):
    def __init__(
        self,
        input_shape,
        latent_dim=20,
        num_hidden_units=512,
        kernel_size=5,
        stride=2,
        act=F.leaky_relu,
        deterministic=False,
    ):
        super().__init__()
        self.encoder = Encoder(
            latent_dim=latent_dim,
            act=act,
            num_hidden_units=num_hidden_units,
            kernel_size=kernel_size,
            stride=stride,
        )
        self.decoder = Decoder(
            output_shape=input_shape,
            latent_dim=latent_dim,
            act=act,
            num_hidden_units=num_hidden_units,
            kernel_size=kernel_size,
            stride=stride,
        )
        self.latent_sampler = StochasticSampler(deterministic=deterministic)
        self.act = act

    def forward(self, x):
        # x shape: [batch_size, sequence_length, num_features]
        z_mean, z_log_var = self.encoder(x)
        z = self.latent_sampler(z_mean, z_log_var)
        reconstructed_x = self.decoder(z)
        # loss to enforce all possible values are sampled from latent space
        # should be of the size of the batch

        # Reconstruction Loss (Mean Squared Error)
        reconstruction_loss = F.mse_loss(reconstructed_x, x, reduction="mean")
        loss_kl = -0.5 * torch.mean(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())
        # / x.size(1)
        # average the KL divergence across the batch dimension
        # In the VAE-LSTM paper, they mention normalizing the KL divergence term by the number of features in the input.
        vae_loss = reconstruction_loss + loss_kl
        return reconstructed_x, vae_loss

# %% ../nbs/01_vae.ipynb 74
@torch.no_grad()
def evaluate_reconstruction(original_signal, reconstructed_signal):
    """
    Evaluates the quality of the reconstructed signal compared to the original signal.

    Args:
        original_signal (torch.Tensor): Original time series signal.
        reconstructed_signal (torch.Tensor): Reconstructed signal from the VAE.

    Returns:
        dict: Dictionary containing the evaluation metrics:
            - mse: Mean Squared Error
            - mae: Mean Absolute Error
    """

    # Ensure tensors are on the CPU and numpy arrays
    original_signal = original_signal.cpu().numpy()
    reconstructed_signal = reconstructed_signal.cpu().numpy()

    # Mean Squared Error
    mse = nn.MSELoss()(
        torch.from_numpy(original_signal), torch.from_numpy(reconstructed_signal)
    ).item()

    # Mean Absolute Error
    mae = nn.L1Loss()(
        torch.from_numpy(original_signal), torch.from_numpy(reconstructed_signal)
    ).item()

    return {
        "mse": mse,
        "mae": mae,
    }

# %% ../nbs/01_vae.ipynb 76
def validate_epoch(dls, scorer):
    """For the full dataloader, calculate the running loss and score"""
    model.eval()
    running_loss = 0.0
    running_score = 0.0
    with torch.no_grad():
        for batch_idx, xs in enumerate(dls):
            # move to device
            xs = xs.to(device)

            # Forward pass
            xs_gen, loss = model(xs)
            # calc score
            score = scorer(xs, xs_gen)["mse"]

            running_loss += loss.item()
            running_score += score
    return running_loss / len(dls), running_score / len(dls)

# %% ../nbs/01_vae.ipynb 77
from fastcore.xtras import partial
import time
