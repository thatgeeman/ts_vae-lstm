"""Train a VAE to encode embeddings of normal behavior in the time series."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_vae.ipynb.

# %% auto 0
__all__ = ['as_df', 'TSDataset', 'Encoder', 'StochasticSampler', 'Decoder', 'VAE', 'validate_epoch', 'evaluate_reconstruction']

# %% ../nbs/01_vae.ipynb 3
# for configs
from fastcore.xtras import Path
import os
from fastcore.xtras import partial
import time

# %% ../nbs/01_vae.ipynb 4
from torch import nn
import torch.nn.functional as F
import torch
from torch.utils.data import Dataset, DataLoader

# %% ../nbs/01_vae.ipynb 10
from dotenv import load_dotenv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime as dt
from .concepts import get_window
from scipy import signal
import math

# %% ../nbs/01_vae.ipynb 23
def as_df(data):
    """Returns data as dataframe with the column named `value`."""
    return pd.DataFrame(data=data, columns=["value"])

# %% ../nbs/01_vae.ipynb 41
class TSDataset(Dataset):
    def __init__(self, data, mean, std, slice_from=0):
        self.data = data
        self.mean = mean
        self.std = std
        self.slice_from = slice_from

    def __getitem__(self, idx):
        # output[channel] = (input[channel] - mean[channel]) / std[channel]
        # ignore the timestamp column
        x = self.data[idx]["subset"][:, self.slice_from :]  # 1024, 4
        normed_X = ((x - self.mean) / (self.std + 1e-10)).astype(np.float32)
        return torch.as_tensor(normed_X)

    def __len__(self):
        return len(self.data)

# %% ../nbs/01_vae.ipynb 53
class Encoder(nn.Module):
    def __init__(
        self,
        in_channels=1,
        latent_dim=20,
        num_hidden_units=512,
        kernel_size=5,
        stride=2,
        act=F.mish,
    ):
        super().__init__()
        self.flatten = nn.Flatten()
        self.conv1 = nn.Conv2d(
            in_channels=in_channels,
            out_channels=num_hidden_units // 8,
            kernel_size=kernel_size,
            stride=stride,
        )
        self.conv2 = nn.Conv2d(
            in_channels=num_hidden_units // 8,
            out_channels=num_hidden_units,
            kernel_size=kernel_size,
            stride=stride,
        )
        self.bn1 = nn.BatchNorm2d(num_hidden_units // 8)
        self.bn2 = nn.BatchNorm2d(num_hidden_units)

        self.linear = nn.LazyLinear(
            out_features=num_hidden_units,
            bias=False,
        )
        self.linear_mean = nn.LazyLinear(
            out_features=latent_dim,
            bias=False,
        )
        self.linear_var = nn.LazyLinear(
            out_features=latent_dim,
            bias=False,
        )
        self.act = act
        self.init_weights()

    def forward(self, x):
        x = x.unsqueeze(1)  # 100, 1, 48, 1
        x = self.act(self.bn1(self.conv1(x)))  # 100, 32, 23, 1
        x = self.act(self.bn2(self.conv2(x)))  # 100, 64, 11, 1
        x = self.flatten(x)  # 100, 512
        x = self.act(self.linear(x))  # 100, 512
        z_mean = self.linear_mean(x)
        z_log_var = self.linear_var(x)
        return z_mean, z_log_var

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                torch.nn.init.xavier_normal_(m.weight)
                m.bias.data.fill_(0)

# %% ../nbs/01_vae.ipynb 58
class StochasticSampler(nn.Module):
    """We basically want to parametrize the sampling from the latent space"""

    def __init__(self, deterministic=False):
        super().__init__()
        self.sampler = torch.distributions.Normal(loc=0, scale=1)
        self.deterministic = deterministic

    def forward(self, z_mean, z_log_var):
        """Return a normal sample value Z from the latent space given a mean and variance"""
        # z_mean and z_log_var are mean and log-var estimates of the latent space
        # under the assumption that the latent space is a gaussian normal
        device = z_mean.device
        # Scales and shifts the sampled values using the reparameterization trick
        eps = self.sampler.sample(z_mean.shape).squeeze().to(device)
        # print(eps.shape, z_log_var.shape, z_mean.shape)
        return (
            z_mean
            if self.deterministic
            else (z_mean + torch.exp(0.5 * z_log_var) * eps)
        )

# %% ../nbs/01_vae.ipynb 64
class Decoder(nn.Module):
    def __init__(
        self,
        out_features,
        latent_dim=20,
        num_hidden_units=512,
        kernel_size=5,
        stride=2,
        act=F.mish,
    ):
        super().__init__()
        assert isinstance(
            out_features, tuple
        ), "out_features must be a tuple of expected output shape"
        self.out_features = out_features
        self.linear = nn.Linear(
            in_features=latent_dim, out_features=num_hidden_units, bias=False
        )
        self.dconv1 = nn.ConvTranspose2d(
            in_channels=num_hidden_units,
            out_channels=num_hidden_units,
            kernel_size=kernel_size,
            stride=stride,
        )
        self.dconv2 = nn.ConvTranspose2d(
            in_channels=num_hidden_units,
            out_channels=num_hidden_units // 8,
            kernel_size=kernel_size,
            stride=stride,
        )

        self.bn1 = nn.BatchNorm2d(num_hidden_units)
        self.bn2 = nn.BatchNorm2d(num_hidden_units // 8)

        self.flatten = nn.Flatten()
        self.linear_out = nn.LazyLinear(
            out_features=math.prod(out_features),
            bias=False,
        )

        self.act = act

        self.init_weights()

    def forward(self, x):
        x = self.linear(x)
        x = x[:, :, None, None]
        x = self.act(self.bn1(self.dconv1(x)))
        x = self.act(self.bn2(self.dconv2(x)))
        x = self.flatten(x)
        # no act for last layer
        x = self.linear_out(x)
        return self.reshape_to_output(x)

    def reshape_to_output(self, x):
        bs = x.shape[0]
        return x.reshape(bs, *self.out_features)

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                torch.nn.init.xavier_normal_(m.weight)
                m.bias.data.fill_(0)

# %% ../nbs/01_vae.ipynb 68
class VAE(nn.Module):
    def __init__(
        self,
        input_shape,
        latent_dim=20,
        num_hidden_units=512,
        kernel_size=5,
        stride=2,
        act=F.leaky_relu,
        deterministic=False,
    ):
        super().__init__()
        self.encoder = Encoder(
            in_channels=input_shape[1],
            latent_dim=latent_dim,
            act=act,
            num_hidden_units=num_hidden_units,
            kernel_size=kernel_size,
            stride=stride,
        )
        self.decoder = Decoder(
            out_features=input_shape,
            latent_dim=latent_dim,
            act=act,
            num_hidden_units=num_hidden_units,
            kernel_size=kernel_size,
            stride=stride,
        )
        self.latent_sampler = StochasticSampler(deterministic=deterministic)
        self.act = act

    def forward(self, x):
        # x shape: [batch_size, sequence_length, num_features]
        z_mean, z_log_var = self.encoder(x)
        z = self.latent_sampler(z_mean, z_log_var)
        reconstructed_x = self.decoder(z)
        # loss to enforce all possible values are sampled from latent space
        # should be of the size of the batch

        # Reconstruction Loss (Mean Squared Error)
        reconstruction_loss = F.mse_loss(reconstructed_x, x, reduction="mean")
        loss_kl = -0.5 * torch.mean(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())
        # / x.size(1)
        # average the KL divergence across the batch dimension
        # In the VAE-LSTM paper, they mention normalizing the KL divergence term by the number of features in the input.
        vae_loss = reconstruction_loss + loss_kl
        return reconstructed_x, vae_loss

# %% ../nbs/01_vae.ipynb 74
def validate_epoch(model, dls, scorer, device="cpu"):
    """For the full dataloader, calculate the running loss and score"""
    model.eval()
    running_loss = 0.0
    running_score = 0.0
    with torch.no_grad():
        for batch_idx, xs in enumerate(dls):
            # move to device
            xs = xs.to(device)

            # Forward pass
            xs_gen, loss = model(xs)
            # calc score
            score = scorer(xs, xs_gen)["mse"]

            running_loss += loss.item()
            running_score += score
    return running_loss / len(dls), running_score / len(dls)

# %% ../nbs/01_vae.ipynb 76
@torch.no_grad()
def evaluate_reconstruction(original_signal, reconstructed_signal):
    """
    Evaluates the quality of the reconstructed signal compared to the original signal.

    Args:
        original_signal (torch.Tensor): Original time series signal.
        reconstructed_signal (torch.Tensor): Reconstructed signal from the VAE.

    Returns:
        dict: Dictionary containing the evaluation metrics:
            - mse: Mean Squared Error
            - mae: Mean Absolute Error
    """

    # Ensure tensors are on the CPU and numpy arrays
    original_signal = original_signal.cpu().numpy()
    reconstructed_signal = reconstructed_signal.cpu().numpy()

    # Mean Squared Error
    mse = nn.MSELoss()(
        torch.from_numpy(original_signal), torch.from_numpy(reconstructed_signal)
    ).item()

    # Mean Absolute Error
    mae = nn.L1Loss()(
        torch.from_numpy(original_signal), torch.from_numpy(reconstructed_signal)
    ).item()

    return {
        "mse": mse,
        "mae": mae,
    }
