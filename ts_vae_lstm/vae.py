# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_vae.ipynb.

# %% auto 0
__all__ = ['model', 'Encoder', 'StochasticSampler', 'VAE', 'calculate_smape', 'loss_func']

# %% ../nbs/01_vae.ipynb 3
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime as dt
from .concepts import get_window
from scipy import signal
import os
import math

# %% ../nbs/01_vae.ipynb 14
from torch import nn
import torch.nn.functional as F
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import roc_auc_score, accuracy_score


# %% ../nbs/01_vae.ipynb 33
# encoder
# l_win to 24, the model would consider each 24-hour period as one sequence.
# pad: if your array is [1, 2, 3] and you symmetrically pad it with 1 unit, the result would be [2, 1, 2, 3, 2].
# xavier_initializer()
# conv 1: num_hidden_units / 16
# conv 2: num_hidden_units / 8
# conv 3: num_hidden_units / 4
# conv 4: num_hidden_units / 1, kernel = 4, 1
# padding : same


class Encoder(nn.Module):
    def __init__(
        self,
        latent_dim=20,
        num_hidden_units=512,
        kernel_size=(3, 1),
        stride=(2, 1),
        act=F.mish,
    ):
        super().__init__()
        self.flatten = nn.Flatten()
        self.conv1 = nn.Conv2d(
            in_channels=1,
            out_channels=num_hidden_units // 16,
            kernel_size=kernel_size,
            stride=stride,
        )
        self.conv2 = nn.Conv2d(
            in_channels=num_hidden_units // 16,
            out_channels=num_hidden_units // 8,
            kernel_size=kernel_size,
            stride=stride,
        )
        self.conv3 = nn.Conv2d(
            in_channels=num_hidden_units // 8,
            out_channels=num_hidden_units // 4,
            kernel_size=kernel_size,
            stride=stride,
        )
        self.conv4 = nn.Conv2d(
            in_channels=num_hidden_units // 4,
            out_channels=num_hidden_units,
            kernel_size=(4, 1),
            stride=stride,
        )
        self.linear = nn.Linear(
            in_features=num_hidden_units, out_features=num_hidden_units, bias=True
        )
        self.linear_mean = nn.Linear(
            in_features=num_hidden_units, out_features=latent_dim, bias=True
        )
        self.linear_var = nn.Linear(
            in_features=num_hidden_units, out_features=latent_dim, bias=True
        )
        self.act = act
        self.init_weights()

    def forward(self, x):
        x = x.unsqueeze(1)  # 100, 1, 48, 1
        x = self.act(self.conv1(x))  # 100, 32, 23, 1
        x = self.act(self.conv2(x))  # 100, 64, 11, 1
        x = self.act(self.conv3(x))  # 100, 128, 5, 1
        x = self.act(self.conv4(x))  # 100, 512, 1, 1
        x = self.flatten(x)  # 100, 512
        x = self.act(self.linear(x))  # 100, 512
        z_mean = self.act(self.linear_mean(x))
        z_log_var = self.act(self.linear_var(x))
        return z_mean, z_log_var

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                torch.nn.init.xavier_normal_(m.weight)
                m.bias.data.fill_(0)
            elif isinstance(m, nn.Conv2d):
                torch.nn.init.xavier_normal_(m.weight)
                m.bias.data.fill_(0)


# %% ../nbs/01_vae.ipynb 37
class StochasticSampler(nn.Module):
    def __init__(self):
        super().__init__()
        self.sampler = torch.distributions.Normal(loc=0, scale=1)

    def forward(self, z_mean, z_log_var):
        # z_mean and z_log_var are mean and log-var estimates of the latent space
        # under the assumption that the latent space is a gaussian normal
        device = z_mean.device
        eps = self.sampler.sample(z_mean.shape).squeeze().to(device)
        # print(eps.shape, z_log_var.shape, z_mean.shape)
        return z_mean + torch.exp(0.5 * z_log_var) * eps

# %% ../nbs/01_vae.ipynb 45
class VAE(nn.Module):
    def __init__(self, latent_dim=20, input_shape=(window_size, 1), act=F.leaky_relu):
        super().__init__()
        self.encoder = Encoder(latent_dim=latent_dim, act=act)
        self.decoder = Decoder(latent_dim=latent_dim, output_shape=input_shape, act=act)
        self.latent_sampler = StochasticSampler()
        self.act = act

    def forward(self, x):
        z_mean, z_log_var = self.encoder(x)
        z = self.latent_sampler(z_mean, z_log_var)
        out = self.decoder(z)
        # loss to enforce all possible values are sampled from latent space
        # should be of the size of the batch
        loss_kl = -0.5 * torch.sum(
            1 + z_log_var - z_mean.pow(2) - z_log_var.exp(), dim=-1
        )
        return out, loss_kl

# %% ../nbs/01_vae.ipynb 48
def calculate_smape(predicted, actual):
    absolute_percentage_errors = (
        torch.abs(predicted - actual) / (torch.abs(predicted) + torch.abs(actual))
    ) * 100
    smape = absolute_percentage_errors
    return smape

# %% ../nbs/01_vae.ipynb 49
model = None


def loss_func(inputs, targets, loss_kl):
    # targets = torch.where(targets >= 0, 1., 0.)
    bs = inputs.shape[0]
    # loss_kl = loss_kl.unsqueeze(-1)  # add loss_kl per time step.
    loss_reconstruct_numerical = F.l1_loss(
        inputs,
        targets,
        reduction="none",
    ).mean((1, 2))
    # loss only for the signal
    # loss_reconstruct = calculate_smape(inputs, targets).mean((1, 2))
    # should be of the size of the batch to add losses correctly
    # loss_kl of shape bs,
    # loss_reconstruct of shape bs,
    """print(
        loss_kl.shape,
        loss_reconstruct_numerical.shape,
        # loss_reconstruct_categorical.shape,
    )"""
    return torch.mean(loss_reconstruct_numerical + loss_kl)

