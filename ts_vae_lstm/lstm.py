# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_lstm.ipynb.

# %% auto 0
__all__ = ['BASEDIR', 'MODELDIR', 'get_embeddings', 'TSLSTMDataset', 'LSTMModel']

# %% ../nbs/02_lstm.ipynb 3
from dotenv import load_dotenv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime as dt
from .concepts import get_window
from scipy import signal
import os
import math
import torch

# %% ../nbs/02_lstm.ipynb 6
load_dotenv()

BASEDIR = os.getenv("BASEDIR")
MODELDIR = os.getenv("MODELDIR")

# %% ../nbs/02_lstm.ipynb 8
from torch import nn
import torch.nn.functional as F
import torch
from torch.utils.data import Dataset, DataLoader

# %% ../nbs/02_lstm.ipynb 9
from .vae import VAE, Encoder, Decoder, StochasticSampler
from fastcore.xtras import noop

# %% ../nbs/02_lstm.ipynb 16
@torch.no_grad()
def get_embeddings(x, n_windows=1, latent_dim=32, seq_len=1, sampler_repeat=200):
    """
    _summary_

    Parameters
    ----------
    x : _type_
        _description_
    n_windows : int, optional
        _description_, by default 1
    latent_dim : int, optional
        _description_, by default 32
    seq_len : int, optional
        _description_, by default 1
    sampler_repeat : int, optional
        Number of times to repeatedly sample from the sampler to ensure we have enough variablity in the embedding, by default 10

    Returns
    -------
    _type_
        _description_
    """
    # actual_shape = x.shape[0]
    vae_model.eval()
    x = torch.from_numpy(x.astype(np.float32)).view(
        n_windows, -1, seq_len
    )  # p, seq -> n_windows, p, seq
    embedded_x = torch.zeros(n_windows, latent_dim, seq_len)
    for idx in range(n_windows):
        batched_x_window = x[idx].unsqueeze(0)
        # print(batched_x_window.shape)
        z_mean, z_log_var = vae_model.encoder(batched_x_window)
        # print(z_mean.shape, z_log_var.shape)
        for _ in range(sampler_repeat):
            # explore multiple potential future embeddings by sampling from the latent space multiple times (Monte Carlo sampling).
            embedded_x[idx] += (
                vae_model.latent_sampler(z_mean, z_log_var).permute(1, 0)
                / sampler_repeat
            )
    # reshape
    embedded_x = embedded_x.reshape(latent_dim * n_windows, -1)
    return embedded_x  # is of shape (n_windows* latent_dim, seq_len)

# %% ../nbs/02_lstm.ipynb 38
class TSLSTMDataset(Dataset):
    def __init__(
        self,
        embeddings,  # full dataset (not separated as we need to index the next window)
        indices,
        # window_size=48,
        # latent_dim=32,
        # n_features=1,
        # n_prev_windows=1,
        mean=0,
        std=1,
    ):
        self.embeddings = embeddings
        self.indices = indices  # (idx, idx+1) pairs)
        self.mean = mean
        self.std = std
        # self.n_prev_windows = n_prev_windows
        # self.window_size = window_size
        # self.n_features = n_features
        # self.latent_dim = latent_dim

    def __getitem__(self, idx):
        xidx = self.indices[idx]
        # per-window statistics could be anopter way to do this.
        seq_emb = (self.embeddings[xidx]["subset"] - self.mean) / self.std
        x_emb = seq_emb[:-1]
        y_emb = seq_emb[1:]
        assert len(x_emb) == len(y_emb)
        return x_emb, y_emb  # latent_dim, seq_len

    def __len__(self):
        return len(self.indices)

# %% ../nbs/02_lstm.ipynb 50
class LSTMModel(nn.Module):
    def __init__(
        self,
        input_size=32,
        hidden_size=128,
        output_size=32,
        activation=F.mish,
        bidirectional=True,
    ):
        super(LSTMModel, self).__init__()
        self.isz = input_size
        self.hsz = hidden_size
        self.osz = output_size
        self.lstm_input = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,  # output_size,
            batch_first=True,
            bidirectional=bidirectional,
            num_layers=1,
        )
        self.lstm_hidden = nn.LSTM(
            input_size=hidden_size * 2
            if bidirectional
            else hidden_size,  # output_size,
            hidden_size=hidden_size * 2 if bidirectional else hidden_size,
            batch_first=True,
            bidirectional=bidirectional,
            num_layers=1,
        )
        self.lstm_output = nn.LSTM(
            input_size=hidden_size * 4 if bidirectional else hidden_size,
            hidden_size=output_size,
            num_layers=1,
            batch_first=True,
        )
        self.ln1 = nn.LayerNorm(hidden_size * 2 if bidirectional else hidden_size)
        self.ln2 = nn.LayerNorm(hidden_size * 4 if bidirectional else hidden_size)
        self.activation = activation
        self.dropout = nn.Dropout(0.5)
        self._init_lstm_weights()

    def forward(self, x):
        # x has shape bs, emb_dim, seq_len (emb_dim=latent_dim, seq_len=1)
        x = x.permute(0, 2, 1)  # bs, seq_len, emb_dim
        # LSTM1
        lstm_out1, _ = self.lstm_input(x)
        lstm_out1 = self.dropout(self.activation(self.ln1(lstm_out1)))
        # print(lstm_out1.shape)
        # LSTM2
        lstm_out2, _ = self.lstm_hidden(lstm_out1)
        lstm_out2 = self.dropout(self.activation(self.ln2(lstm_out2)))
        # LSTM3
        lstm_out3, _ = self.lstm_output(lstm_out2)
        return lstm_out3.permute(0, 2, 1)  # bs, output_size, seq_len

    def _init_lstm_weights(self):
        for layer in [self.lstm_input, self.lstm_hidden, self.lstm_output]:
            for name, param in layer.named_parameters():
                if "weight" in name:
                    nn.init.xavier_normal_(param)
                elif "bias" in name:
                    nn.init.constant_(param, 0.0)

# %% ../nbs/02_lstm.ipynb 61
from fastcore.xtras import partial
