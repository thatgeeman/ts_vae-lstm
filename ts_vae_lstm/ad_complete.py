"""Fill in a module description here"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_ad_complete.ipynb.

# %% auto 0
__all__ = ['predict_next_embeddings', 'reconstruct_ts', 'AD']

# %% ../nbs/03_ad_complete.ipynb 3
import pandas as pd
import logging
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime as dt

from scipy import signal
import os
import math
import torch

# %% ../nbs/03_ad_complete.ipynb 4
from torch import nn
import torch.nn.functional as F
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import roc_auc_score, accuracy_score

# %% ../nbs/03_ad_complete.ipynb 5
from .vae import VAE, Encoder, Decoder, StochasticSampler
from .lstm import get_embeddings, LSTMModel, concat_first_emb
from .concepts import get_window
from fastcore.xtras import noop, flatten
from dotenv import load_dotenv

# %% ../nbs/03_ad_complete.ipynb 24
@torch.no_grad()
def predict_next_embeddings(emb, lstm_model):
    lstm_model.eval()
    next_emb = lstm_model(emb)
    return next_emb


@torch.no_grad()
def reconstruct_ts(emb, vae_model):
    vae_model.eval()
    ts = vae_model.decoder(emb)
    return ts

# %% ../nbs/03_ad_complete.ipynb 25
@torch.no_grad()
def AD(
    x,
    y,
    vae_model,
    lstm_model,
    window_size=48,
    # threshold=0.8,
    quantile_thresh=0.99,
    latent_dim=24,
    reconstruct_with_true=False,
    stats={"lstm": (0, 1), "vae": (0, 1)},
    normalize=True,
    sampler_repeat=200,
    # device="cpu",
):
    x = np.asanyarray(x)
    ad_granular = []
    if normalize:
        y = (y - stats["vae"][0]) / stats["vae"][1]
    remainder = len(y) % window_size
    if remainder > 0:
        logging.info(
            f"passed window not a multiple of window_size! Adjusting for {remainder} extra points."
        )
        y = y[remainder:]
        x = x[remainder:]
    # start
    n_windows = int(len(y) / window_size)
    logging.info(f"Number of windows from time series: {n_windows}")
    ad_score = np.zeros(n_windows)
    ad_status = np.zeros((n_windows, window_size))
    ad_window_loc = np.zeros((n_windows, window_size))
    # threshold_t = threshold / window_size  # threshold for each time step in the window
    # window = torch.as_tensor(window).to(device=device).unsqueeze(0)
    # print(window.shape)
    emb = get_embeddings(
        y,
        n_windows=n_windows,
        vae_model=vae_model,
        latent_dim=latent_dim,
        sampler_repeat=sampler_repeat,
    )  # encode 48 steps to 24 embeddings
    n_features = emb.size(-1)
    emb = emb.reshape(1, n_windows, latent_dim, n_features)
    # standardize with training stats
    # print(emb.shape)
    ts = np.zeros((n_windows, window_size))
    y = y.reshape(ts.shape)
    x = x.reshape(ts.shape)
    for widx in range(n_windows):
        emb_idx = emb[:, widx, :, :]
        if normalize:
            emb_idx = (emb_idx - stats["lstm"][0]) / stats["lstm"][1]
        # pass embeddings from 1 to 23, predict 2 to 24
        next_emb = predict_next_embeddings(emb_idx[:, :-1, :], lstm_model=lstm_model)
        # print(next_emb.shape)
        # now can use either the true embeddings for 1-23 and append last prediction of emb
        # or take true embeddings for 1 and prepend to predicted 2-24 emb
        if reconstruct_with_true:
            next_emb = concat_first_emb(
                next_emb[:, -1, :].unsqueeze(0), first_emb=emb_idx[:, :-1, :], dim=1
            )
        else:
            next_emb = concat_first_emb(
                next_emb, first_emb=emb_idx[:, 0, :].unsqueeze(0), dim=1
            )
        next_emb = next_emb[:, :, 0]  # last index not need for decoder
        # VAE decoder
        ts_widx = reconstruct_ts(next_emb, vae_model=vae_model)
        ts_widx = ts_widx.squeeze().detach().numpy()
        ts[widx, :] = ts_widx
        # calculate for this window the anomaly score
        ad_score[widx] = np.linalg.norm(y[widx] - ts_widx, 2)
        # ad_status[widx, :] = ad_score[widx] > threshold  # global label for all items
        ad_window_loc[widx, :] = x[widx, :]

    if normalize:
        ts = ts * stats["vae"][1] + stats["vae"][0]
        y = y * stats["vae"][1] + stats["vae"][0]
    # global label for whole window
    threshold = np.quantile(ad_score, q=quantile_thresh)
    logging.info(f"global thresh from Q{quantile_thresh}: {threshold}")
    ad_status = np.repeat(ad_score > threshold, repeats=window_size).reshape(
        n_windows, window_size
    )
    # within the detected windows, look for fine grained points of anomaly
    for widx in range(n_windows):
        if ad_score[widx] > threshold:
            _diff = np.mean(
                (y[widx].reshape(-1, 1) - ts_widx.reshape(-1, 1)) ** 2,
                axis=-1,
            )
            threshold_t = np.quantile(_diff, q=quantile_thresh)
            # if the absolute difference is greater than a certain quantile, mark anomaly
            anomaly_markers = 1 * (_diff > threshold_t)
            # only pick the first time when anomaly was spotted
            nzidx = np.nonzero(anomaly_markers)[0][0]
            ad_granular.append(x[widx, nzidx])

    logging.info(f"count anomalies (grainular): {len(ad_granular)}")
    return {
        "reconstructed": ts,
        "actual": y,
        "score": ad_score,
        "status": ad_status,
        "status_granular": ad_granular,
        "steps": x,
        "threshold": threshold,
    }
