# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_ad_complete.ipynb.

# %% auto 0
__all__ = ['TSLSTMDataset', 'loss_func_lstm', 'huber_loss', 'mean_absolute_error', 'calculate_smape', 'scorer_lstm']

# %% ../nbs/03_ad_complete.ipynb 3
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime as dt
from .concepts import get_window
from scipy import signal
import os
import math
import torch

# %% ../nbs/03_ad_complete.ipynb 16
from torch import nn
import torch.nn.functional as F
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import roc_auc_score, accuracy_score

# %% ../nbs/03_ad_complete.ipynb 25
from .vae import VAE, Encoder, Decoder, StochasticSampler
from fastcore.xtras import noop


# %% ../nbs/03_ad_complete.ipynb 33
class TSLSTMDataset(Dataset):
    def __init__(
        self,
        data,
        vae_model,
        window_size=48,
        latent_dim=128,
        n_features=1,
        n_prev_windows=2,
        mean=0,
        std=1,
        emb_mean=0,
        emb_std=1,
    ):
        self.data = data
        self.vae_model = vae_model
        self.mean = mean
        self.std = std
        self.emb_mean = emb_mean
        self.emb_std = emb_std
        self.n_prev_windows = n_prev_windows
        self.window_size = window_size
        self.latent_dim = latent_dim
        self.n_features = n_features

    def __getitem__(self, idx):
        # assuming normed dataset
        # output[channel] = (input[channel] - mean[channel]) / std[channel]
        # ignore the timestamp column
        n_prev_windows = self.n_prev_windows
        x_subset = np.concatenate(
            [
                self.data[_idx]["subset"][:, slice_from:]
                for _idx in range(idx, idx + n_prev_windows)
            ]
        )
        y_subset = self.data[idx + n_prev_windows + 1]["subset"][:, slice_from:]
        x = torch.as_tensor(self.standardize(x_subset, self.mean, self.std)).to(
            torch.float32
        )
        y = (
            torch.as_tensor(self.standardize(y_subset, self.mean, self.std))
            .unsqueeze(0)
            .to(torch.float32)
        )
        x_emb, y_emb = self.get_embeddings(x), self.get_embeddings(y)
        x_emb = self.standardize(x_emb, self.emb_mean, self.emb_std).to(torch.float32)
        y_emb = self.standardize(y_emb, self.emb_mean, self.emb_std).to(torch.float32)
        return x_emb, y_emb  # 20, 1

    def __len__(self):
        return len(self.data) - (
            self.n_prev_windows + 2
        )  # so that ValueError can be prevented

    def get_embeddings(self, x):
        actual_shape = x.shape[0]  # number of datapoints
        latent_dim = self.latent_dim
        window_size = self.window_size
        n_features = self.n_features
        n_windows = x.shape[0] // window_size
        n_windows = n_windows + 1 if n_windows == 0 else n_windows
        x = x.reshape(n_windows, window_size, -1) if n_windows > 1 else x
        emb_val = torch.zeros((n_windows, latent_dim, n_features))
        with torch.no_grad():
            for idx in range(n_windows):
                # print(x[idx].shape)
                z_mean, z_var = vae_model.encoder(
                    x[idx].unsqueeze(0)
                )  # unsqueeze for vae model
                emb_val[idx] = vae_model.latent_sampler(z_mean, z_var).permute(
                    1, 0
                )  # 1, 20 -> 20, 1
            # reshape
            emb_val = (
                emb_val.reshape(latent_dim * n_windows, -1)
                if n_windows > 1
                else emb_val[0]
            )
            return emb_val

    def standardize(self, x, m, s):
        return (x - m) / (s)


# %% ../nbs/03_ad_complete.ipynb 51
def loss_func_lstm(inputs, targets):
    bs = inputs.shape[0]
    distance = torch.norm(targets - inputs, dim=2)
    loss = torch.where(distance > 1, distance.pow(2), torch.clamp(1 - distance, min=0))
    return loss.sum((-1)).mean()
    # return loss


def huber_loss(inputs, targets):
    bs = inputs.shape[0]
    loss = F.huber_loss(inputs, targets, reduction="mean", delta=3)
    return loss  # sum((-1, -2)).mean()
    # return loss


def mean_absolute_error(predictions, targets):
    error = torch.abs(predictions - targets)
    mae = torch.sum(error, dim=(-1, -2))
    return torch.mean(mae)


def calculate_smape(predicted, actual):
    absolute_percentage_errors = torch.abs(predicted - actual) / (
        torch.abs(predicted) + torch.abs(actual)
    )
    return absolute_percentage_errors.sum((-1, -2)).mean()


def scorer_lstm(inputs, targets):
    with torch.no_grad():
        return torch.pow(inputs.squeeze() - targets.squeeze(), 2).mean()

# %% ../nbs/03_ad_complete.ipynb 55
from fastcore.xtras import partial
